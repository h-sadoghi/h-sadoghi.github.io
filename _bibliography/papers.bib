---
---
@article{bahraini2022bayesian,
  abbr={SP},
  title={Bayesian framework selection for hyperspectral image denoising},
  author={Bahraini, Tahereh and Ebrahimi-Moghadam, Abbas and Khademi, Morteza and Yazdi, Hadi Sadoghi},
  journal={Signal Processing},
  volume={201},
  pages={108712},
  year={2022},
  google_scholar_id={DUooU5lO8OsC},
  publisher={Elsevier}
}

@article{bahraini2022density,
  abbr={ESA},
  title={Density-oriented linear discriminant analysis},
  author={Bahraini, Tahereh and Hosseini, Seyed Mohammad and Ghasempour, Mahbubeh and Yazdi, Hadi Sadoghi},
  journal={Expert Systems with Applications},
  volume={187},
  pages={115946},
  year={2022},
  google_scholar_id={AHdEip9mkN0C},
  publisher={Elsevier}
}

@article{alhaidery2022cloning,
  abbr={MTA},
  title={Cloning detection scheme based on linear and curvature scale space with new false positive removal filters},
  author={Alhaidery, Manaf Mohammed Ali and Taherinia, Amir Hossein and Yazdi, Hadi Sadoghi},
  journal={Multimedia Tools and Applications},
  volume={81},
  number={6},
  pages={8745-8766},
  year={2022},
  google_scholar_id={sJsF-0ZLhtgC},
  publisher={Springer}
}

@incollection{nayyeri2023randomized,
  abbr={ISCSS},
  title={Randomized Constructive Neural Network Based on Regularized Minimum Error Entropy},
  author={Nayyeri, Mojtaba and Yazdi, Hadi Sadoghi and Rouhani, Modjtaba and Maskooki, Alaleh and M{\"a}kel{\"a}, Marko M},
  booktitle={Impact of Scientific Computing on Science and Society},
  pages={255--274},
  year={2023},
  google_scholar_id={prdVHNxh-e8C},
  publisher={Springer}
}

@article{aghajanzadeh2023task,
  abbr={PR},
  title={Task weighting based on particle filter in deep multi-task learning with a view to uncertainty and performance},
  author={Aghajanzadeh, Emad and Bahraini, Tahereh and Mehrizi, Amir Hossein and Yazdi, Hadi Sadoghi},
  journal={Pattern Recognition},
  volume={140},
  pages={109587},
  year={2023},
  google_scholar_id={VN7nJs4JPk0C},
  publisher={Elsevier}
}

@article{gharib2023management,
  abbr={IS},
  title={Management of the optimizer's curse concept in single-task diffusion networks},
  author={Gharib, Atieh and Sadoghi-Yazdi, Hadi and Taherinia, Amir Hossein},
  journal={Information Sciences},
  volume={642},
  pages={119109},
  year={2023},
  google_scholar_id={BrOSOlqYqPUC},
  publisher={Elsevier}
}

@article{eghdami2022sparsity,
  abbr={ES},
  title={Sparsity-aware support vector data description reinforced by expectation maximization},
  author={Eghdami, Mahdie and Sadoghi Yazdi, Hadi and Salehi, Neshat},
  journal={Expert Systems},
  volume={39},
  number={1},
  pages={e12794},
  year={2022},
  google_scholar_id={wvYxNZNCP7wC},
  publisher={Wiley Online Library}
}

@article{bahraini2022edge,
  abbr={ASC},
  title={Edge preserving range image smoothing using hybrid locally kernel-based weighted least square},
  author={Bahraini, Tahereh and Hamedani, Taha and Hosseini, Seyed Mohammad and Yazdi, Hadi Sadoghi},
  journal={Applied Soft Computing},
  volume={125},
  pages={109234},
  year={2022},
  google_scholar_id={wvYxNZNCP7wC},
  publisher={Elsevier}
}

@article{keshvari2022listmap,
  abbr={IPM},
  title={ListMAP: Listwise learning to rank as maximum a posteriori estimation},
  author={Keshvari, Sanaz and Ensan, Faezeh and Yazdi, Hadi Sadoghi},
  journal={Information Processing \& Management},
  volume={59},
  number={4},
  pages={102962},
  year={2022},
  google_scholar_id={UuEBAcK4md4C},
  publisher={Elsevier}
}

@article{ansari2023diffusion,
  abbr={SP},
  title={Diffusion-based Kalman iterative thresholding for compressed sampling recovery over network},
  author={Ansari-Ram, Fahimeh and Ebrahimi-Moghadam, Abbas and Khademi, Morteza and Sadoghi-Yazdi, Hadi},
  journal={Signal Processing},
  volume={202},
  pages={108750},
  year={2023},
  google_scholar_id={mel-f30kHHgC},
  publisher={Elsevier}
}

@article{bahraini2021modified,
  abbr={CG},
  title={Modified-mean-shift-based noisy label detection for hyperspectral image classification},
  author={Bahraini, Tahereh and Azimpour, Peyman and Yazdi, Hadi Sadoghi},
  journal={Computers \& Geosciences},
  volume={155},
  pages={104843},
  year={2021},
  google_scholar_id={YB4bud6kWLwC},
  publisher={Elsevier}
}




@article{Mehrizi2016_GSOM,
  abbr={IDA}, 
  title={Semi-supervised GSOM integrated with extreme learning machine}, 
  author={Ali Mehrizi, Hadi Sadoghi Yazdi}, 
  abstract={Semi-supervised learning with a growing self-organizing map (GSOM) is commonly used to cope with the machine
learning problems...}, 
  journal={Intelligent Data Analysis}, 
  volume={20}, 
  pages={1115}, 
  year={2016}, 
  month={June}, 
  publisher={IOS press}, 
  doi={10.3233/IDA-160859}, 
  url={https://content.iospress.com/articles/intelligent-data-analysis/ida859}, 
  google_scholar_id={LK8CI43ZvvMCJ}, 
  selected={true} 
}

@article{Mehrizi2018_RobustGSOM,
  abbr={ESA}, 
  title={Robust semi-supervised growing self-organizing map}, 
  author={Ali Mehrizi, Hadi Sadoghi Yazdi, Amir Hossein Taherinia}, 
  abstract={This paper presents a robust semi-supervised growing self-organizing map (GSOM) approach. The proposed method aims to address the challenges faced in machine learning tasks, specifically classification in the presence of both labeled and unlabeled data. By integrating unsupervised and supervised learning components, the proposed GSOM method enhances the adaptability and accuracy of learning, even with a limited amount of labeled data. The robustness of the proposed approach is demonstrated through extensive experiments across several benchmark datasets, showing superior performance compared to traditional semi-supervised techniques.}, 
  journal={Expert Systems with Applications}, 
  volume={105}, 
  pages={23-33}, 
  year={2018}, 
  month={August}, 
  publisher={Elsevier}, 
  doi={10.1016/j.eswa.2018.03.046}, 
  url={https://www.sciencedirect.com/science/article/abs/pii/S0957417418301921?via%3Dihub}, 
  google_scholar_id={Wq2b2clWBLsC}, 
  selected={true} 
}


@article{NikKhorasani_Mehrizi_2024_RobustHybridLearning,
  abbr={FSS}, 
  title={Robust hybrid learning approach for adaptive neuro-fuzzy inference systems}, 
  author={Ali Nik-Khorasani and Ali Mehrizi and Hadi Sadoghi-Yazdi}, 
  abstract={The Adaptive Neuro-Fuzzy Inference System (ANFIS) is a regression model that combines fuzzy logic and neural networks, making it suitable for modeling the uncertainty of regression problems. However, the non-robust loss function in ANFIS's hybrid learning algorithm can render it vulnerable to the direct effects of noise and outliers. This paper introduces a new procedure that utilizes robust loss functions to enhance the hybrid learning performance against noise and outliers. Furthermore, a new robust loss function is developed that can completely ignore outliers. A set of robust loss functions with mathematical relations are also suggested. The proposed approach is evaluated on real-world problems, including weather forecasting and stock market prediction. Results indicate that the proposed model can reduce the Mean Square Error (MSE) in regression.}, 
  journal={Fuzzy Sets and Systems}, 
  volume={481}, 
  pages={108890}, 
  year={2024}, 
  month={April}, 
  publisher={North-Holland}, 
  doi={10.1016/j.fss.2024.108890}, 
  google_scholar_id={L_l9e5I586QC}, 
  selected={true} 
}

@article{Mehrizi2024_EnhancingMultiTargetTracking,
  abbr={MTA}, 
  title={Enhancing multi-target tracking stability using knowledge graph integration within the Gaussian Mixture Probability Hypothesis Density Filter}, 
  author={Ali Mehrizi and Hadi Sadoghi Yazdi}, 
  abstract={This paper proposes a novel approach to enhancing multi-target tracking of vehicles in videos with frequent camera occlusions. Our method integrates prior knowledge about vehicle behavior into a Gaussian Mixture Probability Hypothesis Density (GMPHD) filter framework. This knowledge, extracted as a knowledge graph from historical vehicle trajectories, allows the tracker to maintain persistence even during significant interruptions. The knowledge graph models expected movement patterns and generates pseudo-observations during occlusions, similar to how time series analysis leverages historical data for forecasting. We evaluate the proposed method on both simulated and real-world video datasets using the Optimal Sub Pattern Assignment (OSPA) metric, which assesses tracking accuracy. The results show a 19.5\% improvement for simulated data and a 16.5\% improvement for real-world video data under ...}, 
  journal={Multimedia Tools and Applications}, 
  pages={1-23}, 
  year={2024}, 
  month={September}, 
  publisher={Springer US}, 
  doi={10.1007/s11042-024-20180-4}, 
  google_scholar_id={OzeSX8-yOCQC}, 
  selected={true} 
}

@article{keshvari2024self,
  abbr={ACM TIS},
  title={A Self-Distilled Learning to Rank Model for Ad-hoc Retrieval},
  author={Keshvari, Sanaz and Saeedi, Farzan and Sadoghi Yazdi, Hadi and Ensan, FAEZEH},
  journal={ACM Transactions on Information Systems},
  year={2024},
  google_scholar_id={yxmsSjX2EkcC},
  publisher={ACM New York, NY}
}

@article{barani2024distributed,
  abbr={JPDC},
  title={A distributed learning based on robust diffusion SGD over adaptive networks with noisy output data},
  author={Barani, Fatemeh and Savadi, Abdorreza and Yazdi, Hadi Sadoghi},
  journal={Journal of Parallel and Distributed Computing},
  volume={190},
  pages={104883},
  year={2024},
  google_scholar_id={zGdJYJv2LkUC},
  publisher={Elsevier}
}

@article{nayyeri2024correntropy,
  abbr={Algorithms},
  title={Correntropy-based constructive one hidden layer neural network},
  author={Nayyeri, Mojtaba and Rouhani, Modjtaba and Yazdi, Hadi Sadoghi and M{\"a}kel{\"a}, Marko M and Maskooki, Alaleh and Nikulin, Yury},
  journal={Algorithms},
  volume={17},
  number={1},
  pages={49},
  year={2024},
  google_scholar_id={-DxkuPiZhfEC},
  publisher={MDPI}
}

@article{j.eswa.2022.116811, 
  abbr={ESA}, 
  title={Robust classification via clipping-based kernel recursive least lncosh of error}, 
  author={Alireza Naeimi-Sadigh, Tahereh Bahraini, Hadi Sadoghi Yazdi}, 
  abstract={Classification as a supervised machine learning method predicts the class label from the features distribution and the training process. The traditional classifier algorithms are not robust in the presence of outliers and noisy features. In this study, we suggest a novel robust classifier using kernel recursive least lncosh (RC-KRLL), in which the clipping concept is used to ignore the effect of large noises. Instead of the conventional mean square error cost function, the suggested RC-KRLL method is derived from the lncosh loss function, being more suitable for non-Gaussian noise. The mean, mean-square convergence, and learning curve of the RC-KRLL are discussed theoretically. The simulation results show that the proposed method outperforms the other robust state-of-the-art classification algorithms in the presence of synthetic non-Gaussian noise over UCI data sets, such as the mixture of Gaussian noise with different variances, impulse, and Alpha–Beta noises. The obtained results over 500px data sets on social media with real outliers and mislabeled samples confirmed the acceptable performance of the proposed method.}, 
  journal={Expert Systems with Applications}, 
  location={}, 
  volume={198}, 
  issue={}, 
  pages={116811}, 
  numpages={0}, 
  year={2022}, 
  month={July}, 
  publisher={elsevier}, 
  doi={10.1016/j.eswa.2022.116811}, 
  url={https://www.sciencedirect.com/science/article/pii/S0957417422002688}, 
  html={https://www.sciencedirect.com/science/article/pii/S0957417422002688}, 
  pdf={}, 
  altmetric={}, 
  dimensions={true}, 
  google_scholar_id={4vMrXwiscB8C}, 
  video={}, 
  additional_info={}, 
  annotation={}, 
  selected={true} 
}
@article{j.sigpro.2020.107950, 
  abbr={SP}, 
  title={Diversity-based diffusion robust RLS using adaptive forgetting factor}, 
  author={Alireza Naeimi-Sadigh, Hadi Sadoghi Yazdi, Ahad Harati}, 
  abstract={}, 
  journal={Signal Processing}, 
  location={}, 
  volume={182}, 
  issue={}, 
  pages={107950}, 
  numpages={0}, 
  year={2021}, 
  month={May}, 
  publisher={elsevier}, 
  doi={10.1016/j.sigpro.2020.107950}, 
  url={https://www.sciencedirect.com/science/article/pii/S0165168420304941}, 
  html={https://www.sciencedirect.com/science/article/pii/S0165168420304941}, 
  pdf={}, 
  altmetric={}, 
  dimensions={true}, 
  google_scholar_id={fFSKOagxvKUC}, 
  video={}, 
  additional_info={}, 
  annotation={}, 
  selected={true} 
}
@article{j.sigpro.2020.107482, 
  abbr={SP}, 
  title={Analysis of robust recursive least squares: Convergence and tracking}, 
  author={Alireza Naeimi-Sadigh, Amir Hossein Taherinia, Hadi Sadoghi Yazdi}, 
  abstract={Outliers and impulsive noise are inevitable factors in recursive least squares (RLS). Developing robust RLS is vital in practical applications such as system identification in which outliers in the desired signals may severely divert the solutions. Almost all suggested robust RLS schemes have been designed for the impulsive noise and Gaussian environments. Recently, employing the Maximum Correntropy Criterion (MCC), the RGMCC (Recursive General MCC) algorithm has been given which yields more exact results for system identification problem in non-Gaussian environments. Here, we develop a new Robust RLS (R2LS) scheme based on the MCC. In contrast to RGMCC, the structure of our model, although being complex, makes it possible to conduct convergence and performance analysis in both the stationary and non-stationary environments. Especially, the model is capable to reasonably predict and track the signals when the original signal is contaminated by non-Gaussian noise. To establish convergence and performance, we apply a half-quadratic optimization algorithm in the multiplicative form to successively convert our model to a quadratic problem which can be effectively solved by the classical tools. Numerical experiments are done on real and synthetic datasets; they show that the proposed algorithm outperforms the conventional RLS as well as some of its recent extensions.}, 
  journal={Signal Processing}, 
  location={}, 
  volume={171}, 
  issue={}, 
  pages={107482}, 
  numpages={0}, 
  year={2021}, 
  month={May}, 
  publisher={elsevier}, 
  doi={10.1016/j.sigpro.2020.107482}, 
  url={https://www.sciencedirect.com/science/article/pii/S0165168420300256}, 
  html={https://www.sciencedirect.com/science/article/pii/S0165168420300256}, 
  pdf={}, 
  altmetric={}, 
  dimensions={true}, 
  google_scholar_id={kVjdVfd2voEC}, 
  video={}, 
  additional_info={}, 
  annotation={}, 
  selected={true} 
}
@article{j.isatra.2020.05.025, 
  abbr={ISA Transactions}, 
  title={Convergence and performance analysis of kernel regularized robust recursive least squares}, 
  author={Alireza Naeimi-Sadigh, Hadi Sadoghi Yazdi, Ahad Harati}, 
  abstract={Kernel recursive least squares (KRLS) is very sensitive to non-Gaussian noise and hence, robust extensions are proposed using maximum correntropy criterion or generalized maximum correntropy. However, because of the complex form of the model, there is no theoretical analysis on the convergence of these filters. In this paper, we propose a new alternative: Kernel Regularized Robust RLS (KRLS). It uses half-quadratic technique to simplify the form of the loss function. Our major contribution is then proving the convergence of the filter to the target weights and desired output. The bounds of regularization factor is also obtained. KRLS is experimentally tested using synthetic and real data and is shown to perform superior compared to other robust alternatives.}, 
  journal={ISA Transactions}, 
  location={}, 
  volume={105}, 
  issue={}, 
  pages={396-405}, 
  numpages={0}, 
  year={2020}, 
  month={October}, 
  publisher={elsevier}, 
  doi={10.1016/j.isatra.2020.05.025}, 
  url={https://www.sciencedirect.com/science/article/pii/S0019057820302019}, 
  html={https://www.sciencedirect.com/science/article/pii/S0019057820302019}, 
  pdf={}, 
  altmetric={}, 
  dimensions={true}, 
  google_scholar_id={cK4Rrx0J3m0C}, 
  video={}, 
  additional_info={}, 
  annotation={}, 
  selected={true} 
}
@article{j.engappai.2024.107928, 
  abbr={EAAI}, 
  title={Semantic labeling of social big media using distributed online robust classification}, 
  author={Alireza Naeimi-Sadigh, Tahereh Bahraini, Hadii Sadoghi Yazdi}, 
  abstract={Semantic labeling for image datasets is of significant importance in a wide range of social media. However, social datasets with massive amounts of data require effective technologies to increase the quality of classification. In this study, we propose a novel online robust classification using distributed learning method, in which the diffusion method is used over adaptive networks in order to the parallelization of the training process. The loss function of the suggested method is derived from the logarithm cosine function, being more suitable for impulsive noises. Also, the convergence of the proposed method is discussed theoretically. The Extensive experimental results show that the proposed method outperforms the other robust state-of-the-art classification algorithms in the presence of various scenarios, such as: free noise synthetic data sets, pure Gaussian noise, mixture of two Gaussian noises, impulse noise, and Alpha–Beta noise which are added to the synthetic data sets. In addition, the experiments were repeated for two different types of real data sets with real noises and outliers, i.e., UCI and 500PX social media data sets. The obtained results over UCI and 500px data sets on social media with real outliers and mislabeled samples confirmed the acceptable performance of the proposed method.}, 
  journal={Engineering Applications of Artificial Intelligence}, 
  location={}, 
  volume={132}, 
  issue={}, 
  pages={107928}, 
  numpages={0}, 
  year={2024}, 
  month={June}, 
  publisher={elsevier}, 
  doi={10.1016/j.engappai.2024.107928}, 
  url={https://www.sciencedirect.com/science/article/pii/S0952197624000861}, 
  html={https://www.sciencedirect.com/science/article/pii/S0952197624000861}, 
  pdf={}, 
  altmetric={}, 
  dimensions={true}, 
  google_scholar_id={eO3_k5sD8BwC}, 
  video={}, 
  additional_info={}, 
  annotation={}, 
  selected={true} 
}

@article{ j.patcog.2014.11.005,  
         abbr={PR},   
         title={ IRAHC: instance reduction algorithm using hyperrectangle clustering },   
         author={ Javad Hamidzadeh, Reza Monsefi, Hadi Sadoghi Yazdi },   
         abstract={ In instance-based classifiers, there is a need for storing a large number of samples as training set. In this work, we propose an instance reduction method based on hyperrectangle clustering, called Instance Reduction Algorithm using Hyperrectangle Clustering (IRAHC). IRAHC removes non-border (interior) instances and keeps border and near border ones. This paper presents an instance reduction process based on hyperrectangle clustering. A hyperrectangle is an n-dimensional rectangle with axes aligned sides, which is defined by min and max points and a corresponding distance function. The min–max points are determined by using the hyperrectangle clustering algorithm. Instance-based learning algorithms are often confronted with the problem of deciding which instances must be stored to be used during an actual test. Storing too many instances can result in a large memory requirements and a slow execution speed. In IRAHC, core of instance reduction process is based on set of hyperrectangles. The performance has been evaluated on real world data sets from UCI repository by the 10-fold cross-validation method. The results of the experiments have been compared with state-of-the-art methods, which show superiority of the proposed method in terms of classification accuracy and reduction percentage.},   
         journal={ Pattern Recognition},   
         location={},   
         volume={48},   
         issue={5},   
         pages={1878-1889},   
         numpages={12},   
         year={2015},   
         month={May},   
         publisher={elsevier},   
         doi={ 10.1016/j.patcog.2014.11.005},   
         url={ https://www.sciencedirect.com/science/article/abs/pii/S0031320314004555},   
         html={ https://www.sciencedirect.com/science/article/abs/pii/S0031320314004555},   
         pdf={},   
         altmetric={},   
         dimensions={true},   
         google_scholar_id={ghEM2AJqZyQC},   
         video={},   
         additional_info={},   
         annotation={},   
         selected={true} 
}

@article{ s13042-014-0239-z,  
       abbr={IJMLC},  
       title={ Large symmetric margin instance selection algorithm},  
       author={ Javad Hamidzadeh, Reza Monsefi, Hadi Sadoghi Yazdi},  
       abstract={ In instance-based classifiers, there is a need for storing a large number of samples as a training set. In this paper, we propose a large symmetric margin instance selection algorithm, namely LAMIS. LAMIS removes non-border (interior) instances and keeps border ones. This paper presents an instance selection process through formulating it as a constrained binary optimization problem and solves it by employment filled function algorithm. Instance-based learning algorithms are often confronted with the problem of deciding which instances must be stored for use during an actual test. Storing too many instances can result in large memory requirements and slow execution. In LAMIS, the core of instance selection process is based on keeping the hyperplane that separates a two-class data, to provide large margin separation. LAMIS selects the most representative instances, satisfying both objectives: high accuracy and reduction rates. The performance has been evaluated on real world data sets from UCI repository by the ten-fold cross-validation method. The results of experiments have been compared with state-of-the-art methods, where the overall results, show the superiority of the proposed method in terms of classification accuracy and reduction percentage.},  
       journal={ International Journal of Machine Learning and Cybernetics},  
       location={},  
       volume={7},  
       issue={5},  
       pages={ 25-45 },  
       numpages={21},  
       year={2016},  
       month={Feb},  
       publisher={ Springer},  
       doi={ 10.1007/s13042-014-0239-z},  
       url={ https://link.springer.com/article/10.1007/s13042-014-0239-z},  
       html={ https://link.springer.com/article/10.1007/s13042-014-0239-z},  
       pdf={},  
       altmetric={},  
       dimensions={true},  
       google_scholar_id={-mN3Mh-tlDkC},  
       video={},  
       additional_info={},  
       annotation={},  
       selected={true}  
     } 

@article{ j.neucom.2014.05.006,  
       abbr={Neurocomputing},  
       title={ LMIRA: large margin instance reduction algorithm},  
       author={ Javad Hamidzadeh, Reza Monsefi, Hadi Sadoghi Yazdi},  
       abstract={ In instance-based learning, a training set is given to a classifier for classifying new instances. In practice, not all information in the training set is useful for classifiers. Therefore, it is convenient to discard irrelevant instances from the training set. This process is known as instance reduction, which is an important task for classifiers since through this process the time for classification or training could be reduced. In this paper, we propose a Large Margin Instance Reduction Algorithm, namely LMIRA. LMIRA removes non-border instances and keeps border ones. In the proposed method, the instance reduction process is formulated as a constrained binary optimization problem and then it is solved by employing a filled function algorithm. Instance-based learning algorithms are often confronted with the difficulty of choosing those instances which must be stored to be used during an actual test. Storing too many instances can result in large memory requirements and slow execution. In LMIRA, core of instance reduction process is based on keeping the hyperplane that separates a two-class data and provides large margin separation. LMIRA selects the most representative instances, satisfying both following objectives: high accuracy and reduction rates. The performance has been evaluated on real world data sets from UCI repository by the ten-fold cross-validation method. The results of experiments are compared with state-of-the-art methods, which show the superiority of proposed method in terms of classification accuracy and reduction percentage.},  
       journal={ Neurocomputing},  
       location={},  
       volume={145},  
       issue={},  
       pages={ 477-487 },  
       numpages={11},  
       year={2014},  
       month={Dec},  
       publisher={ elsevier },  
       doi={ 10.1016/j.neucom.2014.05.006},  
       url={ https://www.sciencedirect.com/science/article/abs/pii/S0925231214005682 },  
       html={ https://www.sciencedirect.com/science/article/abs/pii/S0925231214005682 },  
       pdf={},  
       altmetric={},  
       dimensions={true},  
       google_scholar_id={zdjWy_NXXwUC},  
       video={},  
       additional_info={},  
       annotation={},  
       selected={true}  
     }

     @article{ s00521-011-0762-8,  
       abbr={NCA},  
       title={ DDC: distance-based decision classifier},  
       author={ Javad Hamidzadeh, Reza Monsefi, Hadi Sadoghi Yazdi},  
       abstract={ This paper presents a new classification method utilizing distance-based decision surface with nearest neighbor projection approach, called DDC. Kernel type of DDC has been extended to take into account the effective nonlinear structure of the data. DDC has some properties: (1) does not need conventional learning procedure (as k-NN algorithm), (2) does not need searching time to locate the k-nearest neighbors, and (3) does not need optimization process unlike some classification methods such as Support Vector Machine (SVM). In DDC, we compute the weighted average of distances to all the training samples. Unclassified sample will be classified as belonging to a class that has the minimum obtained distance. As a result, by such a rule we can derive a formula that can be used as the decision surface. DDC is tested on both synthetic and real-world data sets from the UCI repository, and the results were compared with k-NN, RBF Network, and SVM. The experimental results indicate DDC outperforms k-NN in the most experiments and the results are comparable to or better than SVM with some data sets.},  
       journal={ Neural Computing and Applications },  
       location={},  
       volume={21},  
       issue={},  
       pages={ 1697-1707 },  
       numpages={11},  
       year={2012},  
       month={Nov},  
       publisher={ Springer },  
       doi={ 10.1007/s00521-011-0762-8},  
       url={ https://link.springer.com/article/10.1007/s00521-011-0762-8 },  
       html={ https://link.springer.com/article/10.1007/s00521-011-0762-8 },  
       pdf={},  
       altmetric={},  
       dimensions={true},  
       google_scholar_id={nqdriD65xNoC},  
       video={},  
       additional_info={},  
       annotation={},  
       selected={true}  
     }

@article{toussi2011feature,
  abbr={IJSPIPPR},
  title={Feature selection in spectral clustering},
  author={Toussi, Soheila Ashkezari and Yazdi, Hadi Sadoghi},
  abstract={Spectral clustering is a powerful technique in clustering specially when the structure of data is not linear and classical clustering methods lead to fail. In this paper, we propose a spectral clustering algorithm with a feature selection schema based on extracted features of Kernel PCA. In the proposed algorithm, selecting appropriate vectors is dependent upon entropy of clusters on these vectors and weighting method is influenced by sum of the existence gap between clusters and entropy of the vectors. Tuning the parameters has a great effect on the results of spectral clustering techniques. In the ideal case, comparing our method with NJW and Kernel K-Means indicate the successful of the proposed algorithm.},
  journal={International Journal of Signal Processing, Image Processing and Pattern Recognition},
  location={}, 
  volume={4},
  number={3}, 
  pages={179-194},
  numpages={16}, 
  year={2011},
  month={}, 
  publisher={}, 
  doi={}, 
  url={}, 
  html={}, 
  pdf={}, 
  altmetric={}, 
  dimensions={true}, 
  google_scholar_id={IaI1MmNe2tcC}, 
  video={}, 
  additional_info={}, 
  annotation={}, 
  selected={true}
}

@article{ashkezari2019emotional,
  abbr={Cities},
  title={Emotional maps based on social networks data to analyze cities' emotional structure and measure their emotional similarity},
  author={Ashkezari-Toussi, Soheila and Kamel, Mohammad and Sadoghi-Yazdi, Hadi},
  abstract={The present study utilizes the social media data to sample from people's emotion in different places based on their facial expressions, for analyzing emotions distribution in a city and comparing the emotional similarity between cities. Experiencing various emotions g1ives birth to different facial expressions which usually have similar patterns for individuals. Extracting these patterns from face images and analyzing them makes it possible to extract related emotion associated with the shared photos. Since geo-tagged images include metadata about the geographical location, estimation of emotions spatial distribution is possible. The distribution of four emotions: anger, disgust, happiness, and surprise in twelve cities, included Athens, Beijing, Berlin, Brussels, Buenos Aires, Copenhagen, Helsinki, Melbourn, New York, Ottawa, Paris, and Prague is investigated by analyzing geo-tagged photos shared on Flickr. After facial expressions extraction, the spatial distribution functions of the emotions are calculated by kernel density estimation method. Emotional maps for cities are created using extracted features of Fourier transform spectrum of the spatial distributions which are robust against rotation and translation. Afterward, similar cities based on their emotional structure are clustered into appropriate groups. The results are beneficial for urban planners and social researchers to analyze the effect of the environment on peoples' emotions. Furthermore, this could guide them to make right policies to improve the quality of life. Considering the four studied emtions, New York has the highest ratio in anger, Ottawa has the highest proportion of happiness and Copenhagen has the highest ratio in both surprise and disgust among the studied cities. In addition, an attempt is made to calculate the percentage of different emotions and dispersion of them as important emotional factors in understanding the emotional structure of cities. To measure the dispersion of happiness the coefficient of variation is computed. The results show that happiness dispersion in Berlin has a more uniform distribution according to the collected data. It seems both coefficients of variation and emotions percentage may be jointly effective in urban planning and cities comparison based on people emotional mode.},
  journal={Cities},
  location={}, 
  volume={86},
  issue={}, 
  pages={113-124},
  numpages={12}, 
  year={2019},
  month={January}, 
  publisher={Pergamon},
  doi={10.1016/j.cities.2018.09.009},
  url={https://www.sciencedirect.com/science/article/abs/pii/S0264275117312519},
  html={https://www.sciencedirect.com/science/article/abs/pii/S0264275117312519},
  pdf={}, 
  altmetric={}, 
  dimensions={true}, 
  google_scholar_id={C33y2ycGS3YC}, 
  video={}, 
  additional_info={}, 
  annotation={}, 
  selected={true}
}

@article{ashkezari2019robust,
  abbr={SP},
  title={Robust diffusion LMS over adaptive networks},
  author={Ashkezari-Toussi, Soheila and Sadoghi-Yazdi, Hadi},
  abstract={The present study proposes the Robust DLMS (RDLMS) algorithm for a robust estimation over adaptive networks. Instead of minimizing the mean square error (MSE), the RDLMS algorithm is derived from minimizing the pseudo-Huber function which is a continuous derivative and smooth approximation of the Huber function. Performance of the RDLMS algorithm is examined in the presence of Gaussian and α-stable non-Gaussian noise, in stationary and non-stationary environments. The results show that in the presence of non-Gaussian noise the proposed algorithm is robust and outperforms the diffusion LMS, the diffusion maximum correntropy criteria, and the diffusion least mean fourth algorithms. In addition, RDLMS is similar to the diffusion sign-error LMS and diffusion robust LMS. On the other hand, when the environment noise is Gaussian, the performance of RDLMS is similar to the DLMS while outperforms the other aforementioned algorithms.},
  journal={Signal Processing},
  location={}, 
  volume={158},
  number={}, 
  pages={201-209},
  numpages={9}, 
  year={2019},
  month={July}, 
  publisher={Elsevier},
  doi={10.1016/j.sigpro.2019.01.004}, 
  url={https://www.sciencedirect.com/science/article/abs/pii/S0165168419300131},
  html={https://www.sciencedirect.com/science/article/abs/pii/S0165168419300131},
  pdf={}, 
  altmetric={}, 
  dimensions={true}, 
  google_scholar_id={KWzIFqRkAKkC}, 
  video={}, 
  additional_info={}, 
  annotation={}, 
  selected={true}
}

@article{ashkezari2019incorporating,
  abbr={CSSP},
  title={Incorporating Nonparametric Knowledge to the Least Mean Square Adaptive Filter},
  author={Ashkezari-Toussi, Soheila and Sadoghi-Yazdi, Hadi},
  abstract={In the framework of the maximum a posteriori estimation, the present study proposes the nonparametric probabilistic least mean square (NPLMS) adaptive filter for the estimation of an unknown parameter vector from noisy data. The NPLMS combines parameter space and signal space by combining the prior knowledge of the probability distribution of the process with the evidence existing in the signal. Taking advantage of kernel density estimation to estimate the prior distribution, the NPLMS is robust against the Gaussian and non-Gaussian noises. To achieve this, some of the intermediate estimations are buffered and then used to estimate the prior distribution. Despite the bias-compensated algorithms, there is no need to estimate the input noise variance. Theoretical analysis of the NPLMS is derived. In addition, a variable step-size version of NPLMS is provided to reduce the steady-state error. Simulation results in the system identification and prediction show the acceptable performance of the NPLMS in the noisy stationary and non-stationary environments against the bias-compensated and normalized LMS algorithms.},
  journal={Circuits, Systems, and Signal Processing},
  location={}, 
  volume={38},
  number={}, 
  pages={2114-2137},
  numpages={24}, 
  year={2019},
  month={April}, 
  publisher={Springer US},
  doi={10.1007/s00034-018-0954-x}, 
  url={https://link.springer.com/article/10.1007/s00034-018-0954-x},
  html={https://link.springer.com/article/10.1007/s00034-018-0954-x},
  pdf={}, 
  altmetric={}, 
  dimensions={true}, 
  google_scholar_id={AXkvAH5U_nMC}, 
  video={}, 
  additional_info={}, 
  annotation={}, 
  selected={true}
}

@article{yazdi2011edge,
  abbr={IJSPIPPR},
  title={Edge/corner programming},
  author={Yazdi, Hadi Sadoghi and Toussi, Soheila Ashkezari},
  abstract={Many vision tasks are based on edge/corner detection. We propose some rules and utilize them to find suitable window for edge/corner detection by using quadratic programming. Experimental results over synthetic images and real images show satisfactory outcomes.},
  journal={International Journal of Signal Processing, Image Processing and Pattern Recognition},
  location={}, 
  volume={4},
  number={2}, 
  pages={51-64},
  numpages={14}, 
  year={2011},
  month={April}, 
  publisher={}, 
  doi={}, 
  url={}, 
  html={}, 
  pdf={}, 
  altmetric={}, 
  dimensions={true}, 
  google_scholar_id={s9ia6_kGH2AC}, 
  video={}, 
  additional_info={}, 
  annotation={}, 
  selected={true}
}

@article{yazdi2010unsupervised,
  abbr={ASC},
  title={Unsupervised adaptive neural-fuzzy inference system for solving differential equations},
  author={Yazdi, Hadi Sadoghi and Pourreza, Reza},
  journal={Applied Soft Computing},
  volume={10},
  number={1},
  pages={267-275},
  year={2010},
  publisher={Elsevier},
  google_scholar_id={umqufdRvDiIC}
}

@inproceedings{izanloo2016kalman,
  abbr={ISC},
  title={Kalman filtering based on the maximum correntropy criterion in the presence of non-Gaussian noise},
  author={Izanloo, Reza and Fakoorian, Seyed Abolfazl and Yazdi, Hadi Sadoghi and Simon, Dan},
  booktitle={2016 Annual Conference on Information Science and Systems (CISS)},
  pages={500-505},
  year={2016},
  organization={IEEE},
  google_scholar_id={2VqYfGB8ITEC}
}

@article{ghazikhani2013ensemble,
  abbr={Neurocomputing},
  title={Ensemble of online neural networks for non-stationary and imbalanced data streams},
  author={Ghazikhani, Adel and Monsefi, Reza and Yazdi, Hadi Sadoghi},
  journal={Neurocomputing},
  volume={122},
  pages={535-544},
  year={2013},
  publisher={Elsevier},
  google_scholar_id={_OXeSy2IsFwC}
}

@article{rahgooy2009fuzzy,
  abbr={IJCEE},
  title={Fuzzy complex system of linear EquationsApplied to circuit analysis},
  author={Rahgooy, Taher and Yazdi, Hadi Sadoghi and Monsefi, Reza},
  journal={International Journal of Computer and Electrical Engineering},
  volume={1},
  number={5},
  pages={535},
  year={2009},
  publisher={IACSIT Press},
  google_scholar_id={otzGkya1bYkC}
}

