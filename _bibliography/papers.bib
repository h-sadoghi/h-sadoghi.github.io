---
---
@article{j.eswa.2022.116811, 
  abbr={Exp. Sys. with App.}, 
  title={Robust classification via clipping-based kernel recursive least lncosh of error}, 
  author={Alireza Naeimi-Sadigh, Tahereh Bahraini, Hadi Sadoghi Yazdi}, 
  abstract={Classification as a supervised machine learning method predicts the class label from the features distribution and the training process. The traditional classifier algorithms are not robust in the presence of outliers and noisy features. In this study, we suggest a novel robust classifier using kernel recursive least lncosh (RC-KRLL), in which the clipping concept is used to ignore the effect of large noises. Instead of the conventional mean square error cost function, the suggested RC-KRLL method is derived from the lncosh loss function, being more suitable for non-Gaussian noise. The mean, mean-square convergence, and learning curve of the RC-KRLL are discussed theoretically. The simulation results show that the proposed method outperforms the other robust state-of-the-art classification algorithms in the presence of synthetic non-Gaussian noise over UCI data sets, such as the mixture of Gaussian noise with different variances, impulse, and Alpha–Beta noises. The obtained results over 500px data sets on social media with real outliers and mislabeled samples confirmed the acceptable performance of the proposed method.}, 
  journal={Expert Systems with Applications}, 
  location={}, 
  volume={198}, 
  issue={}, 
  pages={116811}, 
  numpages={0}, 
  year={2022}, 
  month={July}, 
  publisher={elsevier}, 
  doi={10.1016/j.eswa.2022.116811}, 
  url={https://www.sciencedirect.com/science/article/pii/S0957417422002688}, 
  html={https://www.sciencedirect.com/science/article/pii/S0957417422002688}, 
  pdf={}, 
  altmetric={}, 
  dimensions={true}, 
  google_scholar_id={4vMrXwiscB8C}, 
  video={}, 
  additional_info={}, 
  annotation={}, 
  selected={true} 
}
@article{j.sigpro.2020.107950, 
  abbr={Signal Processing}, 
  title={Diversity-based diffusion robust RLS using adaptive forgetting factor}, 
  author={Alireza Naeimi-Sadigh, Hadi Sadoghi Yazdi, Ahad Harati}, 
  abstract={}, 
  journal={Signal Processing}, 
  location={}, 
  volume={182}, 
  issue={}, 
  pages={107950}, 
  numpages={0}, 
  year={2021}, 
  month={May}, 
  publisher={elsevier}, 
  doi={10.1016/j.sigpro.2020.107950}, 
  url={https://www.sciencedirect.com/science/article/pii/S0165168420304941}, 
  html={https://www.sciencedirect.com/science/article/pii/S0165168420304941}, 
  pdf={}, 
  altmetric={}, 
  dimensions={true}, 
  google_scholar_id={fFSKOagxvKUC}, 
  video={}, 
  additional_info={}, 
  annotation={}, 
  selected={true} 
}
@article{j.sigpro.2020.107482, 
  abbr={Signal Processing}, 
  title={Analysis of robust recursive least squares: Convergence and tracking}, 
  author={Alireza Naeimi-Sadigh, Amir Hossein Taherinia, Hadi Sadoghi Yazdi}, 
  abstract={Outliers and impulsive noise are inevitable factors in recursive least squares (RLS). Developing robust RLS is vital in practical applications such as system identification in which outliers in the desired signals may severely divert the solutions. Almost all suggested robust RLS schemes have been designed for the impulsive noise and Gaussian environments. Recently, employing the Maximum Correntropy Criterion (MCC), the RGMCC (Recursive General MCC) algorithm has been given which yields more exact results for system identification problem in non-Gaussian environments. Here, we develop a new Robust RLS (R2LS) scheme based on the MCC. In contrast to RGMCC, the structure of our model, although being complex, makes it possible to conduct convergence and performance analysis in both the stationary and non-stationary environments. Especially, the model is capable to reasonably predict and track the signals when the original signal is contaminated by non-Gaussian noise. To establish convergence and performance, we apply a half-quadratic optimization algorithm in the multiplicative form to successively convert our model to a quadratic problem which can be effectively solved by the classical tools. Numerical experiments are done on real and synthetic datasets; they show that the proposed algorithm outperforms the conventional RLS as well as some of its recent extensions.}, 
  journal={Signal Processing}, 
  location={}, 
  volume={171}, 
  issue={}, 
  pages={107482}, 
  numpages={0}, 
  year={2021}, 
  month={May}, 
  publisher={elsevier}, 
  doi={10.1016/j.sigpro.2020.107482}, 
  url={https://www.sciencedirect.com/science/article/pii/S0165168420300256}, 
  html={https://www.sciencedirect.com/science/article/pii/S0165168420300256}, 
  pdf={}, 
  altmetric={}, 
  dimensions={true}, 
  google_scholar_id={kVjdVfd2voEC}, 
  video={}, 
  additional_info={}, 
  annotation={}, 
  selected={true} 
}
@article{j.isatra.2020.05.025, 
  abbr={ISA Transactions}, 
  title={Convergence and performance analysis of kernel regularized robust recursive least squares}, 
  author={Alireza Naeimi-Sadigh, Hadi Sadoghi Yazdi, Ahad Harati}, 
  abstract={Kernel recursive least squares (KRLS) is very sensitive to non-Gaussian noise and hence, robust extensions are proposed using maximum correntropy criterion or generalized maximum correntropy. However, because of the complex form of the model, there is no theoretical analysis on the convergence of these filters. In this paper, we propose a new alternative: Kernel Regularized Robust RLS (KRLS). It uses half-quadratic technique to simplify the form of the loss function. Our major contribution is then proving the convergence of the filter to the target weights and desired output. The bounds of regularization factor is also obtained. KRLS is experimentally tested using synthetic and real data and is shown to perform superior compared to other robust alternatives.}, 
  journal={ISA Transactions}, 
  location={}, 
  volume={105}, 
  issue={}, 
  pages={396-405}, 
  numpages={0}, 
  year={2020}, 
  month={October}, 
  publisher={elsevier}, 
  doi={10.1016/j.isatra.2020.05.025}, 
  url={https://www.sciencedirect.com/science/article/pii/S0019057820302019}, 
  html={https://www.sciencedirect.com/science/article/pii/S0019057820302019}, 
  pdf={}, 
  altmetric={}, 
  dimensions={true}, 
  google_scholar_id={cK4Rrx0J3m0C}, 
  video={}, 
  additional_info={}, 
  annotation={}, 
  selected={true} 
}
@article{j.engappai.2024.107928, 
  abbr={Eng. App. of Art. Int.}, 
  title={Semantic labeling of social big media using distributed online robust classification}, 
  author={Alireza Naeimi-Sadigh, Tahereh Bahraini, Hadii Sadoghi Yazdi}, 
  abstract={Semantic labeling for image datasets is of significant importance in a wide range of social media. However, social datasets with massive amounts of data require effective technologies to increase the quality of classification. In this study, we propose a novel online robust classification using distributed learning method, in which the diffusion method is used over adaptive networks in order to the parallelization of the training process. The loss function of the suggested method is derived from the logarithm cosine function, being more suitable for impulsive noises. Also, the convergence of the proposed method is discussed theoretically. The Extensive experimental results show that the proposed method outperforms the other robust state-of-the-art classification algorithms in the presence of various scenarios, such as: free noise synthetic data sets, pure Gaussian noise, mixture of two Gaussian noises, impulse noise, and Alpha–Beta noise which are added to the synthetic data sets. In addition, the experiments were repeated for two different types of real data sets with real noises and outliers, i.e., UCI and 500PX social media data sets. The obtained results over UCI and 500px data sets on social media with real outliers and mislabeled samples confirmed the acceptable performance of the proposed method.}, 
  journal={Engineering Applications of Artificial Intelligence}, 
  location={}, 
  volume={132}, 
  issue={}, 
  pages={107928}, 
  numpages={0}, 
  year={2024}, 
  month={June}, 
  publisher={elsevier}, 
  doi={10.1016/j.engappai.2024.107928}, 
  url={https://www.sciencedirect.com/science/article/pii/S0952197624000861}, 
  html={https://www.sciencedirect.com/science/article/pii/S0952197624000861}, 
  pdf={}, 
  altmetric={}, 
  dimensions={true}, 
  google_scholar_id={eO3_k5sD8BwC}, 
  video={}, 
  additional_info={}, 
  annotation={}, 
  selected={true} 
}

@article{ j.patcog.2014.11.005,  
         abbr={ Pattern Recognition},   
         title={ IRAHC: instance reduction algorithm using hyperrectangle clustering },   
         author={ Javad Hamidzadeh, Reza Monsefi, Hadi Sadoghi Yazdi },   
         abstract={ In instance-based classifiers, there is a need for storing a large number of samples as training set. In this work, we propose an instance reduction method based on hyperrectangle clustering, called Instance Reduction Algorithm using Hyperrectangle Clustering (IRAHC). IRAHC removes non-border (interior) instances and keeps border and near border ones. This paper presents an instance reduction process based on hyperrectangle clustering. A hyperrectangle is an n-dimensional rectangle with axes aligned sides, which is defined by min and max points and a corresponding distance function. The min–max points are determined by using the hyperrectangle clustering algorithm. Instance-based learning algorithms are often confronted with the problem of deciding which instances must be stored to be used during an actual test. Storing too many instances can result in a large memory requirements and a slow execution speed. In IRAHC, core of instance reduction process is based on set of hyperrectangles. The performance has been evaluated on real world data sets from UCI repository by the 10-fold cross-validation method. The results of the experiments have been compared with state-of-the-art methods, which show superiority of the proposed method in terms of classification accuracy and reduction percentage.},   
         journal={ Pattern Recognition},   
         location={},   
         volume={48},   
         issue={5},   
         pages={1878-1889},   
         numpages={12},   
         year={2015},   
         month={May},   
         publisher={elsevier},   
         doi={ 10.1016/j.patcog.2014.11.005},   
         url={ https://www.sciencedirect.com/science/article/abs/pii/S0031320314004555},   
         html={ https://www.sciencedirect.com/science/article/abs/pii/S0031320314004555},   
         pdf={},   
         altmetric={},   
         dimensions={true},   
         google_scholar_id={ghEM2AJqZyQC},   
         video={},   
         additional_info={},   
         annotation={},   
         selected={true} 
}

@article{ s13042-014-0239-z,  
       abbr={ Int. J. of Machine Learning and Cyb.},  
       title={ Large symmetric margin instance selection algorithm},  
       author={ Javad Hamidzadeh, Reza Monsefi, Hadi Sadoghi Yazdi},  
       abstract={ In instance-based classifiers, there is a need for storing a large number of samples as a training set. In this paper, we propose a large symmetric margin instance selection algorithm, namely LAMIS. LAMIS removes non-border (interior) instances and keeps border ones. This paper presents an instance selection process through formulating it as a constrained binary optimization problem and solves it by employment filled function algorithm. Instance-based learning algorithms are often confronted with the problem of deciding which instances must be stored for use during an actual test. Storing too many instances can result in large memory requirements and slow execution. In LAMIS, the core of instance selection process is based on keeping the hyperplane that separates a two-class data, to provide large margin separation. LAMIS selects the most representative instances, satisfying both objectives: high accuracy and reduction rates. The performance has been evaluated on real world data sets from UCI repository by the ten-fold cross-validation method. The results of experiments have been compared with state-of-the-art methods, where the overall results, show the superiority of the proposed method in terms of classification accuracy and reduction percentage.},  
       journal={ International Journal of Machine Learning and Cybernetics},  
       location={},  
       volume={7},  
       issue={5},  
       pages={ 25-45 },  
       numpages={21},  
       year={2016},  
       month={Feb},  
       publisher={ Springer},  
       doi={ 10.1007/s13042-014-0239-z},  
       url={ https://link.springer.com/article/10.1007/s13042-014-0239-z},  
       html={ https://link.springer.com/article/10.1007/s13042-014-0239-z},  
       pdf={},  
       altmetric={},  
       dimensions={true},  
       google_scholar_id={-mN3Mh-tlDkC},  
       video={},  
       additional_info={},  
       annotation={},  
       selected={true}  
     } 