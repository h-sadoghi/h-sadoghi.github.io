---
---
@article{j.eswa.2022.116811, 
  abbr={Exp. Sys. with App.}, 
  title={Robust classification via clipping-based kernel recursive least lncosh of error}, 
  author={Alireza Naeimi-Sadigh, Tahereh Bahraini, Hadi Sadoghi Yazdi}, 
  abstract={Classification as a supervised machine learning method predicts the class label from the features distribution and the training process. The traditional classifier algorithms are not robust in the presence of outliers and noisy features. In this study, we suggest a novel robust classifier using kernel recursive least lncosh (RC-KRLL), in which the clipping concept is used to ignore the effect of large noises. Instead of the conventional mean square error cost function, the suggested RC-KRLL method is derived from the lncosh loss function, being more suitable for non-Gaussian noise. The mean, mean-square convergence, and learning curve of the RC-KRLL are discussed theoretically. The simulation results show that the proposed method outperforms the other robust state-of-the-art classification algorithms in the presence of synthetic non-Gaussian noise over UCI data sets, such as the mixture of Gaussian noise with different variances, impulse, and Alpha–Beta noises. The obtained results over 500px data sets on social media with real outliers and mislabeled samples confirmed the acceptable performance of the proposed method.}, 
  journal={Expert Systems with Applications}, 
  location={}, 
  volume={198}, 
  issue={}, 
  pages={116811}, 
  numpages={0}, 
  year={2022}, 
  month={July}, 
  publisher={elsevier}, 
  doi={10.1016/j.eswa.2022.116811}, 
  url={https://www.sciencedirect.com/science/article/pii/S0957417422002688}, 
  html={https://www.sciencedirect.com/science/article/pii/S0957417422002688}, 
  pdf={}, 
  altmetric={}, 
  dimensions={true}, 
  google_scholar_id={4vMrXwiscB8C}, 
  video={}, 
  additional_info={}, 
  annotation={}, 
  selected={true} 
}
@article{j.sigpro.2020.107950, 
  abbr={Signal Processing}, 
  title={Diversity-based diffusion robust RLS using adaptive forgetting factor}, 
  author={Alireza Naeimi-Sadigh, Hadi Sadoghi Yazdi, Ahad Harati}, 
  abstract={}, 
  journal={Signal Processing}, 
  location={}, 
  volume={182}, 
  issue={}, 
  pages={107950}, 
  numpages={0}, 
  year={2021}, 
  month={May}, 
  publisher={elsevier}, 
  doi={10.1016/j.sigpro.2020.107950}, 
  url={https://www.sciencedirect.com/science/article/pii/S0165168420304941}, 
  html={https://www.sciencedirect.com/science/article/pii/S0165168420304941}, 
  pdf={}, 
  altmetric={}, 
  dimensions={true}, 
  google_scholar_id={fFSKOagxvKUC}, 
  video={}, 
  additional_info={}, 
  annotation={}, 
  selected={true} 
}
@article{j.sigpro.2020.107482, 
  abbr={Signal Processing}, 
  title={Analysis of robust recursive least squares: Convergence and tracking}, 
  author={Alireza Naeimi-Sadigh, Amir Hossein Taherinia, Hadi Sadoghi Yazdi}, 
  abstract={Outliers and impulsive noise are inevitable factors in recursive least squares (RLS). Developing robust RLS is vital in practical applications such as system identification in which outliers in the desired signals may severely divert the solutions. Almost all suggested robust RLS schemes have been designed for the impulsive noise and Gaussian environments. Recently, employing the Maximum Correntropy Criterion (MCC), the RGMCC (Recursive General MCC) algorithm has been given which yields more exact results for system identification problem in non-Gaussian environments. Here, we develop a new Robust RLS (R2LS) scheme based on the MCC. In contrast to RGMCC, the structure of our model, although being complex, makes it possible to conduct convergence and performance analysis in both the stationary and non-stationary environments. Especially, the model is capable to reasonably predict and track the signals when the original signal is contaminated by non-Gaussian noise. To establish convergence and performance, we apply a half-quadratic optimization algorithm in the multiplicative form to successively convert our model to a quadratic problem which can be effectively solved by the classical tools. Numerical experiments are done on real and synthetic datasets; they show that the proposed algorithm outperforms the conventional RLS as well as some of its recent extensions.}, 
  journal={Signal Processing}, 
  location={}, 
  volume={171}, 
  issue={}, 
  pages={107482}, 
  numpages={0}, 
  year={2021}, 
  month={May}, 
  publisher={elsevier}, 
  doi={10.1016/j.sigpro.2020.107482}, 
  url={https://www.sciencedirect.com/science/article/pii/S0165168420300256}, 
  html={https://www.sciencedirect.com/science/article/pii/S0165168420300256}, 
  pdf={}, 
  altmetric={}, 
  dimensions={true}, 
  google_scholar_id={kVjdVfd2voEC}, 
  video={}, 
  additional_info={}, 
  annotation={}, 
  selected={true} 
}
@article{j.isatra.2020.05.025, 
  abbr={ISA Transactions}, 
  title={Convergence and performance analysis of kernel regularized robust recursive least squares}, 
  author={Alireza Naeimi-Sadigh, Hadi Sadoghi Yazdi, Ahad Harati}, 
  abstract={Kernel recursive least squares (KRLS) is very sensitive to non-Gaussian noise and hence, robust extensions are proposed using maximum correntropy criterion or generalized maximum correntropy. However, because of the complex form of the model, there is no theoretical analysis on the convergence of these filters. In this paper, we propose a new alternative: Kernel Regularized Robust RLS (KRLS). It uses half-quadratic technique to simplify the form of the loss function. Our major contribution is then proving the convergence of the filter to the target weights and desired output. The bounds of regularization factor is also obtained. KRLS is experimentally tested using synthetic and real data and is shown to perform superior compared to other robust alternatives.}, 
  journal={ISA Transactions}, 
  location={}, 
  volume={105}, 
  issue={}, 
  pages={396-405}, 
  numpages={0}, 
  year={2020}, 
  month={October}, 
  publisher={elsevier}, 
  doi={10.1016/j.isatra.2020.05.025}, 
  url={https://www.sciencedirect.com/science/article/pii/S0019057820302019}, 
  html={https://www.sciencedirect.com/science/article/pii/S0019057820302019}, 
  pdf={}, 
  altmetric={}, 
  dimensions={true}, 
  google_scholar_id={cK4Rrx0J3m0C}, 
  video={}, 
  additional_info={}, 
  annotation={}, 
  selected={true} 
}
@article{j.engappai.2024.107928, 
  abbr={Eng. App. of Art. Int.}, 
  title={Semantic labeling of social big media using distributed online robust classification}, 
  author={Alireza Naeimi-Sadigh, Tahereh Bahraini, Hadii Sadoghi Yazdi}, 
  abstract={Semantic labeling for image datasets is of significant importance in a wide range of social media. However, social datasets with massive amounts of data require effective technologies to increase the quality of classification. In this study, we propose a novel online robust classification using distributed learning method, in which the diffusion method is used over adaptive networks in order to the parallelization of the training process. The loss function of the suggested method is derived from the logarithm cosine function, being more suitable for impulsive noises. Also, the convergence of the proposed method is discussed theoretically. The Extensive experimental results show that the proposed method outperforms the other robust state-of-the-art classification algorithms in the presence of various scenarios, such as: free noise synthetic data sets, pure Gaussian noise, mixture of two Gaussian noises, impulse noise, and Alpha–Beta noise which are added to the synthetic data sets. In addition, the experiments were repeated for two different types of real data sets with real noises and outliers, i.e., UCI and 500PX social media data sets. The obtained results over UCI and 500px data sets on social media with real outliers and mislabeled samples confirmed the acceptable performance of the proposed method.}, 
  journal={Engineering Applications of Artificial Intelligence}, 
  location={}, 
  volume={132}, 
  issue={}, 
  pages={107928}, 
  numpages={0}, 
  year={2024}, 
  month={June}, 
  publisher={elsevier}, 
  doi={10.1016/j.engappai.2024.107928}, 
  url={https://www.sciencedirect.com/science/article/pii/S0952197624000861}, 
  html={https://www.sciencedirect.com/science/article/pii/S0952197624000861}, 
  pdf={}, 
  altmetric={}, 
  dimensions={true}, 
  google_scholar_id={eO3_k5sD8BwC}, 
  video={}, 
  additional_info={}, 
  annotation={}, 
  selected={true} 
}

@article{ j.patcog.2014.11.005,  
         abbr={ Pattern Recognition},   
         title={ IRAHC: instance reduction algorithm using hyperrectangle clustering },   
         author={ Javad Hamidzadeh, Reza Monsefi, Hadi Sadoghi Yazdi },   
         abstract={ In instance-based classifiers, there is a need for storing a large number of samples as training set. In this work, we propose an instance reduction method based on hyperrectangle clustering, called Instance Reduction Algorithm using Hyperrectangle Clustering (IRAHC). IRAHC removes non-border (interior) instances and keeps border and near border ones. This paper presents an instance reduction process based on hyperrectangle clustering. A hyperrectangle is an n-dimensional rectangle with axes aligned sides, which is defined by min and max points and a corresponding distance function. The min–max points are determined by using the hyperrectangle clustering algorithm. Instance-based learning algorithms are often confronted with the problem of deciding which instances must be stored to be used during an actual test. Storing too many instances can result in a large memory requirements and a slow execution speed. In IRAHC, core of instance reduction process is based on set of hyperrectangles. The performance has been evaluated on real world data sets from UCI repository by the 10-fold cross-validation method. The results of the experiments have been compared with state-of-the-art methods, which show superiority of the proposed method in terms of classification accuracy and reduction percentage.},   
         journal={ Pattern Recognition},   
         location={},   
         volume={48},   
         issue={5},   
         pages={1878-1889},   
         numpages={12},   
         year={2015},   
         month={May},   
         publisher={elsevier},   
         doi={ 10.1016/j.patcog.2014.11.005},   
         url={ https://www.sciencedirect.com/science/article/abs/pii/S0031320314004555},   
         html={ https://www.sciencedirect.com/science/article/abs/pii/S0031320314004555},   
         pdf={},   
         altmetric={},   
         dimensions={true},   
         google_scholar_id={ghEM2AJqZyQC},   
         video={},   
         additional_info={},   
         annotation={},   
         selected={true} 
}

@article{ s13042-014-0239-z,  
       abbr={ I. J. of M. L. and C.},  
       title={ Large symmetric margin instance selection algorithm},  
       author={ Javad Hamidzadeh, Reza Monsefi, Hadi Sadoghi Yazdi},  
       abstract={ In instance-based classifiers, there is a need for storing a large number of samples as a training set. In this paper, we propose a large symmetric margin instance selection algorithm, namely LAMIS. LAMIS removes non-border (interior) instances and keeps border ones. This paper presents an instance selection process through formulating it as a constrained binary optimization problem and solves it by employment filled function algorithm. Instance-based learning algorithms are often confronted with the problem of deciding which instances must be stored for use during an actual test. Storing too many instances can result in large memory requirements and slow execution. In LAMIS, the core of instance selection process is based on keeping the hyperplane that separates a two-class data, to provide large margin separation. LAMIS selects the most representative instances, satisfying both objectives: high accuracy and reduction rates. The performance has been evaluated on real world data sets from UCI repository by the ten-fold cross-validation method. The results of experiments have been compared with state-of-the-art methods, where the overall results, show the superiority of the proposed method in terms of classification accuracy and reduction percentage.},  
       journal={ International Journal of Machine Learning and Cybernetics},  
       location={},  
       volume={7},  
       issue={5},  
       pages={ 25-45 },  
       numpages={21},  
       year={2016},  
       month={Feb},  
       publisher={ Springer},  
       doi={ 10.1007/s13042-014-0239-z},  
       url={ https://link.springer.com/article/10.1007/s13042-014-0239-z},  
       html={ https://link.springer.com/article/10.1007/s13042-014-0239-z},  
       pdf={},  
       altmetric={},  
       dimensions={true},  
       google_scholar_id={-mN3Mh-tlDkC},  
       video={},  
       additional_info={},  
       annotation={},  
       selected={true}  
     } 

@article{ j.neucom.2014.05.006,  
       abbr={ Neurocomputing},  
       title={ LMIRA: large margin instance reduction algorithm},  
       author={ Javad Hamidzadeh, Reza Monsefi, Hadi Sadoghi Yazdi},  
       abstract={ In instance-based learning, a training set is given to a classifier for classifying new instances. In practice, not all information in the training set is useful for classifiers. Therefore, it is convenient to discard irrelevant instances from the training set. This process is known as instance reduction, which is an important task for classifiers since through this process the time for classification or training could be reduced. In this paper, we propose a Large Margin Instance Reduction Algorithm, namely LMIRA. LMIRA removes non-border instances and keeps border ones. In the proposed method, the instance reduction process is formulated as a constrained binary optimization problem and then it is solved by employing a filled function algorithm. Instance-based learning algorithms are often confronted with the difficulty of choosing those instances which must be stored to be used during an actual test. Storing too many instances can result in large memory requirements and slow execution. In LMIRA, core of instance reduction process is based on keeping the hyperplane that separates a two-class data and provides large margin separation. LMIRA selects the most representative instances, satisfying both following objectives: high accuracy and reduction rates. The performance has been evaluated on real world data sets from UCI repository by the ten-fold cross-validation method. The results of experiments are compared with state-of-the-art methods, which show the superiority of proposed method in terms of classification accuracy and reduction percentage.},  
       journal={ Neurocomputing},  
       location={},  
       volume={145},  
       issue={},  
       pages={ 477-487 },  
       numpages={11},  
       year={2014},  
       month={Dec},  
       publisher={ elsevier },  
       doi={ 10.1016/j.neucom.2014.05.006},  
       url={ https://www.sciencedirect.com/science/article/abs/pii/S0925231214005682 },  
       html={ https://www.sciencedirect.com/science/article/abs/pii/S0925231214005682 },  
       pdf={},  
       altmetric={},  
       dimensions={true},  
       google_scholar_id={zdjWy_NXXwUC},  
       video={},  
       additional_info={},  
       annotation={},  
       selected={true}  
     }

     @article{ s00521-011-0762-8,  
       abbr={ Neu. Comp. and App.},  
       title={ DDC: distance-based decision classifier},  
       author={ Javad Hamidzadeh, Reza Monsefi, Hadi Sadoghi Yazdi},  
       abstract={ This paper presents a new classification method utilizing distance-based decision surface with nearest neighbor projection approach, called DDC. Kernel type of DDC has been extended to take into account the effective nonlinear structure of the data. DDC has some properties: (1) does not need conventional learning procedure (as k-NN algorithm), (2) does not need searching time to locate the k-nearest neighbors, and (3) does not need optimization process unlike some classification methods such as Support Vector Machine (SVM). In DDC, we compute the weighted average of distances to all the training samples. Unclassified sample will be classified as belonging to a class that has the minimum obtained distance. As a result, by such a rule we can derive a formula that can be used as the decision surface. DDC is tested on both synthetic and real-world data sets from the UCI repository, and the results were compared with k-NN, RBF Network, and SVM. The experimental results indicate DDC outperforms k-NN in the most experiments and the results are comparable to or better than SVM with some data sets.},  
       journal={ Neural Computing and Applications },  
       location={},  
       volume={21},  
       issue={},  
       pages={ 1697-1707 },  
       numpages={11},  
       year={2012},  
       month={Nov},  
       publisher={ Springer },  
       doi={ 10.1007/s00521-011-0762-8},  
       url={ https://link.springer.com/article/10.1007/s00521-011-0762-8 },  
       html={ https://link.springer.com/article/10.1007/s00521-011-0762-8 },  
       pdf={},  
       altmetric={},  
       dimensions={true},  
       google_scholar_id={nqdriD65xNoC},  
       video={},  
       additional_info={},  
       annotation={},  
       selected={true}  
     }

@article{toussi2011feature,
  abbr={I.J of SP, IP, PR},
  title={Feature selection in spectral clustering},
  author={Toussi, Soheila Ashkezari and Yazdi, Hadi Sadoghi},
  abstract={Spectral clustering is a powerful technique in clustering specially when the structure of data is not linear and classical clustering methods lead to fail. In this paper, we propose a spectral clustering algorithm with a feature selection schema based on extracted features of Kernel PCA. In the proposed algorithm, selecting appropriate vectors is dependent upon entropy of clusters on these vectors and weighting method is influenced by sum of the existence gap between clusters and entropy of the vectors. Tuning the parameters has a great effect on the results of spectral clustering techniques. In the ideal case, comparing our method with NJW and Kernel K-Means indicate the successful of the proposed algorithm.},
  journal={International Journal of Signal Processing, Image Processing and Pattern Recognition},
  location={}, 
  volume={4},
  number={3}, 
  pages={179--194},
  numpages={16}, 
  year={2011},
  month={}, 
  publisher={}, 
  doi={}, 
  url={}, 
  html={}, 
  pdf={}, 
  altmetric={}, 
  dimensions={true}, 
  google_scholar_id={IaI1MmNe2tcC}, 
  video={}, 
  additional_info={}, 
  annotation={}, 
  selected={true}
}

@article{ashkezari2019emotional,
  abbr={Cities},
  title={Emotional maps based on social networks data to analyze cities' emotional structure and measure their emotional similarity},
  author={Ashkezari-Toussi, Soheila and Kamel, Mohammad and Sadoghi-Yazdi, Hadi},
  abstract={The present study utilizes the social media data to sample from people's emotion in different places based on their facial expressions, for analyzing emotions distribution in a city and comparing the emotional similarity between cities. Experiencing various emotions g1ives birth to different facial expressions which usually have similar patterns for individuals. Extracting these patterns from face images and analyzing them makes it possible to extract related emotion associated with the shared photos. Since geo-tagged images include metadata about the geographical location, estimation of emotions spatial distribution is possible. The distribution of four emotions: anger, disgust, happiness, and surprise in twelve cities, included Athens, Beijing, Berlin, Brussels, Buenos Aires, Copenhagen, Helsinki, Melbourn, New York, Ottawa, Paris, and Prague is investigated by analyzing geo-tagged photos shared on Flickr. After facial expressions extraction, the spatial distribution functions of the emotions are calculated by kernel density estimation method. Emotional maps for cities are created using extracted features of Fourier transform spectrum of the spatial distributions which are robust against rotation and translation. Afterward, similar cities based on their emotional structure are clustered into appropriate groups. The results are beneficial for urban planners and social researchers to analyze the effect of the environment on peoples' emotions. Furthermore, this could guide them to make right policies to improve the quality of life. Considering the four studied emtions, New York has the highest ratio in anger, Ottawa has the highest proportion of happiness and Copenhagen has the highest ratio in both surprise and disgust among the studied cities. In addition, an attempt is made to calculate the percentage of different emotions and dispersion of them as important emotional factors in understanding the emotional structure of cities. To measure the dispersion of happiness the coefficient of variation is computed. The results show that happiness dispersion in Berlin has a more uniform distribution according to the collected data. It seems both coefficients of variation and emotions percentage may be jointly effective in urban planning and cities comparison based on people emotional mode.},
  journal={Cities},
  location={}, 
  volume={86},
  issue={}, 
  pages={113-124},
  numpages={12}, 
  year={2019},
  month={January}, 
  publisher={Pergamon},
  doi={10.1016/j.cities.2018.09.009},
  url={https://www.sciencedirect.com/science/article/abs/pii/S0264275117312519},
  html={https://www.sciencedirect.com/science/article/abs/pii/S0264275117312519},
  pdf={}, 
  altmetric={}, 
  dimensions={true}, 
  google_scholar_id={C33y2ycGS3YC}, 
  video={}, 
  additional_info={}, 
  annotation={}, 
  selected={true}
}

@article{ashkezari2019robust,
  abbr={Signal Process.},
  title={Robust diffusion LMS over adaptive networks},
  author={Ashkezari-Toussi, Soheila and Sadoghi-Yazdi, Hadi},
  abstract={The present study proposes the Robust DLMS (RDLMS) algorithm for a robust estimation over adaptive networks. Instead of minimizing the mean square error (MSE), the RDLMS algorithm is derived from minimizing the pseudo-Huber function which is a continuous derivative and smooth approximation of the Huber function. Performance of the RDLMS algorithm is examined in the presence of Gaussian and α-stable non-Gaussian noise, in stationary and non-stationary environments. The results show that in the presence of non-Gaussian noise the proposed algorithm is robust and outperforms the diffusion LMS, the diffusion maximum correntropy criteria, and the diffusion least mean fourth algorithms. In addition, RDLMS is similar to the diffusion sign-error LMS and diffusion robust LMS. On the other hand, when the environment noise is Gaussian, the performance of RDLMS is similar to the DLMS while outperforms the other aforementioned algorithms.},
  journal={Signal Processing},
  location={}, 
  volume={158},
  number={}, 
  pages={201-209},
  numpages={9}, 
  year={2019},
  month={July}, 
  publisher={Elsevier},
  doi={10.1016/j.sigpro.2019.01.004}, 
  url={https://www.sciencedirect.com/science/article/abs/pii/S0165168419300131},
  html={https://www.sciencedirect.com/science/article/abs/pii/S0165168419300131},
  pdf={}, 
  altmetric={}, 
  dimensions={true}, 
  google_scholar_id={KWzIFqRkAKkC}, 
  video={}, 
  additional_info={}, 
  annotation={}, 
  selected={true}
}

@article{ashkezari2019incorporating,
  abbr={Circ. Syst. SP.},
  title={Incorporating Nonparametric Knowledge to the Least Mean Square Adaptive Filter},
  author={Ashkezari-Toussi, Soheila and Sadoghi-Yazdi, Hadi},
  abstract={In the framework of the maximum a posteriori estimation, the present study proposes the nonparametric probabilistic least mean square (NPLMS) adaptive filter for the estimation of an unknown parameter vector from noisy data. The NPLMS combines parameter space and signal space by combining the prior knowledge of the probability distribution of the process with the evidence existing in the signal. Taking advantage of kernel density estimation to estimate the prior distribution, the NPLMS is robust against the Gaussian and non-Gaussian noises. To achieve this, some of the intermediate estimations are buffered and then used to estimate the prior distribution. Despite the bias-compensated algorithms, there is no need to estimate the input noise variance. Theoretical analysis of the NPLMS is derived. In addition, a variable step-size version of NPLMS is provided to reduce the steady-state error. Simulation results in the system identification and prediction show the acceptable performance of the NPLMS in the noisy stationary and non-stationary environments against the bias-compensated and normalized LMS algorithms.},
  journal={Circuits, Systems, and Signal Processing},
  location={}, 
  volume={38},
  number={}, 
  pages={2114-2137},
  numpages={24}, 
  year={2019},
  month={April}, 
  publisher={Springer US},
  doi={10.1007/s00034-018-0954-x}, 
  url={https://link.springer.com/article/10.1007/s00034-018-0954-x},
  html={https://link.springer.com/article/10.1007/s00034-018-0954-x},
  pdf={}, 
  altmetric={}, 
  dimensions={true}, 
  google_scholar_id={AXkvAH5U_nMC}, 
  video={}, 
  additional_info={}, 
  annotation={}, 
  selected={true}
}

@article{yazdi2011edge,
  abbr={I.J of SP, IP, PR},
  title={Edge/corner programming},
  author={Yazdi, Hadi Sadoghi and Toussi, Soheila Ashkezari},
  abstract={Many vision tasks are based on edge/corner detection. We propose some rules and utilize them to find suitable window for edge/corner detection by using quadratic programming. Experimental results over synthetic images and real images show satisfactory outcomes.},
  journal={International Journal of Signal Processing, Image Processing and Pattern Recognition},
  location={}, 
  volume={4},
  number={2}, 
  pages={51-64},
  numpages={14}, 
  year={2011},
  month={April}, 
  publisher={}, 
  doi={}, 
  url={}, 
  html={}, 
  pdf={}, 
  altmetric={}, 
  dimensions={true}, 
  google_scholar_id={s9ia6_kGH2AC}, 
  video={}, 
  additional_info={}, 
  annotation={}, 
  selected={true}
}